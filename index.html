
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>

  <title>Xun Jiang's Homepage</title>

  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- <meta name="description" content="Lei Wang will be a full-time research scientist at Salesforce AI Research"> -->
  <meta name="keywords" content="Xun Jiang, Multimedia Retrieval, Multimodal Learning, Long-term Video Understanding, and Weakly-Supervised Learning">
  <meta name="author" content="Xun Jiang" />

  <link rel="stylesheet" href="w3.css">

  <style>
  .w3-sidebar a {font-family: "Roboto", sans-serif}
  body,h1,h2,h3,h4,h5,h6,.w3-wide {font-family: "Montserrat", sans-serif;}

  .parentDiv{
    position:relative;
  }

  .div1{
    position:absolute;
    z-index:1;
    border:3px solid #0105fb
  }
  .div2{
      position:absolute;
      z-index:2;
  }
  .education 
  {
      text-align: center;
      vertical-align: middle !important;
  }
  </style>

  <link rel="icon" type="image/png" href="images/favicon.ico">
  <!--
  <script src="jquery.min.js"></script>
  <script>
  $(document).ready(function(){
    // Add smooth scrolling to all links
    $("a").on('click', function(event) {
      // Make sure this.hash has a value before overriding default behavior
      if (this.hash !== "") {
        // Prevent default anchor click behavior
        event.preventDefault();
        // Store hash
        var hash = this.hash;
        // Using jQuery's animate() method to add smooth page scroll
        // The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
        $('html, body').animate({
          scrollTop: $(hash).offset().top
        }, 800, function(){
          // Add hash (#) to URL when done scrolling (default click behavior)
          window.location.hash = hash;
        });
      } // End if
    });
  });
  </script>
  //-->

</head>



<body class="w3-content" style="max-width:1200px">

<!-- Sidebar/menu -->
<nav class="w3-sidebar w3-bar-block w3-blue-grey w3-collapse w3-top w3-right" style="z-index:3;width:150px" id="mySidebar">
  <div class="w3-container w3-display-container w3-padding-16">
    <h3><b><p style="text-align: center;">Xun</p></b></h3>
  </div>
  <div class="w3-padding-64 w3-text-light-grey w3-large" style="font-weight:bold">
    <a href="#home" class="w3-bar-item w3-button">Home</a>
    <a href="#news" class="w3-bar-item w3-button">News</a>
    <a href="#education" class="w3-bar-item w3-button">Education</a>
    <!-- <a href="#projects" class="w3-bar-item w3-button">Projects</a>
    <a href="#talks" class="w3-bar-item w3-button">Talks</a> -->
    <a href="#publications" class="w3-bar-item w3-button">Publications</a>
    <!-- <a href="#service" class="w3-bar-item w3-button">Services</a> -->
    <a href="#award" class="w3-bar-item w3-button">Awards</a>
  </div>
</nav>

<!-- Top menu on small screens -->
<header class="w3-bar w3-top w3-hide-large w3-blue-grey w3-xlarge">
  <div class="w3-bar-item w3-padding-24">Xun</div>
  <a href="javascript:void(0)" class="w3-bar-item w3-button w3-padding-24 w3-right"  style="font-stretch: extra-expanded;" onclick="w3_open()"><b>≡</b></a>
  </div>
</header>

<!-- Overlay effect when opening sidebar on small screens -->
<div class="w3-overlay w3-hide-large" onclick="w3_close()" style="cursor:pointer" title="close side menu" id="myOverlay"></div>

<!-- !PAGE CONTENT! -->
<div class="w3-main" style="margin-left:150px">

  <!-- Push down content on small screens -->
  <div class="w3-hide-large" style="margin-top:83px"></div>

<!-- The Home Section -->
    <div class="w3-container w3-center w3-padding-32" id="home">
      <img style="width: 80%;max-width: 320px" alt="profile photo" src="images/xun_jiang.png">
      <h1>Xun Jiang</h1>
        <p class="w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:800px">
          I am a third-year Ph.D student in Successive Postgraduate and Doctoral Program at <a href="http://cfm.uestc.edu.cn">CFM lab</a> of the University of Electronic and Science of China (UESTC), supervised by <a href="https://cfm.uestc.edu.cn/~shenht/">Prof. Heng Tao Shen</a>, co-supervised by <a href="https://cfm.uestc.edu.cn/~fshen/">Prof. Fumin Shen</a> and <a href="https://interxuxing.github.io/">Prof. Xing Xu</a>. Before that, I earned my bachelor's degree in Software Engineering from UESTC in 2020, where I was recognized as an Honor Graduate. Now, I am a visiting Ph.D student of <a href="https://mreallab.github.io/">MReal Lab</a> at Nanyang Technological University under the supervision of <a href="https://personal.ntu.edu.sg/hanwangzhang/">Prof. Hanwang Zhang</a>. 
        </p>
        <p class="w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:800px">
          My research interests include but are not limited to, Multimedia Retrieval, Multimodal Learning, Long-term Video Understanding, Weakly-Supervised Learning, and VLM/LLM Applications. Currently, I am serving as a reviewer for IEEE TIP, IEEE TMM, IEEE TCSVT, ACM TOIS, ACM TOMM, CVPR, ICCV, ACM MM, AAAI, WWW, ICME, ICMR, <em>etc.</em>. 
        </p>
        <p class="w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:800px">
          Feel free to contact me if you are interested in discussing my research topics!
        </p>
        <p class="w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:800px">
          Email: xun_jiang@outlook.com 
        </p>
        <!-- <p class="w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:600px">
          I am expected to graduate in May 2024. After graduation, I will be joining Salesforce Research in Singapore as a full-time research scientist. 
        </p> -->
        <p class="w3-center">
          <a href="https://scholar.google.com/citations?user=oAjQIUEAAAAJ">Google Scholar</a> &nbsp/&nbsp
          <a href="https://github.com/XunCHN/"> GitHub </a>
        </p>
        </tbody></table>
  </div>




  <div class="w3-container w3-padding-32" id="education">
    <h2>Education</h2>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
        
        <tr></tr>
          <td width="20%" class="education">
            <img src='images/ntu_icon.png' width="70">
            <br \>
            Dec. 2024 - Present
          </td>

          <td width="75%" valign="left">
            <div style="white-space:pre"><strong>Nanyang Technological University (NTU), Singapore</strong></div>
            <div style="white-space:pre"><li> College of Computing and Data Science </div>
            <div style="white-space:pre"><li> Visiting Ph.D Student in Computer Science and Technology </div>
            <em><li> Awarded CSC Scholarship</em>
          </td>
        </tr>

        <tr>
          <td width="20%" class="education">
            <img src='images/uestc_icon.jpg' width="90">
            <br \>
            Dec. 2020 - Present
          </td>

          <td width="75%" valign="left">
            <div style="white-space:pre"><strong>University of Electronic Science and Technology of China (UESTC), China</strong></div>
            <div style="white-space:pre"><li> School of Computer Science and Engineering </div>
            <div style="white-space:pre"><li> Ph.D Student in Computer Science and Technology</div>
            <em> <li> In Successive Postgraduate and Doctoral Program </em>
          </td>
        </tr>


        <tr></tr>
          <td width="20%" class="education">
            <img src='images/uestc_icon.jpg' width="90">
            <br \>
            Sep. 2016 - Jun. 2020
          </td>

          <td width="75%" valign="left">
            <div style="white-space:pre"><strong>University of Electronic Science and Technology of China (UESTC), China</strong></div>
            <div style="white-space:pre"><li> School of Software Engineering</div>
            <div style="white-space:pre"><li> Bachelor Degree in Software Engineering</div>
            <em> <li> Awarded Honor Graduates of UESTC </em>
          </td>
        </tr>
<!-- 
        <tr>
          <td width="10%" class="education">
            <img src='images/uestc_icon.jpg' width="90">
          </td>

          <td width="85%" valign="left">
            <div style="white-space:pre"><strong>University of Electronic Science and Technology of China (UESTC), China</strong></div>
            <div style="white-space:pre">Bachelor Degree in Software Engineering                    Sep. 2016 - Jun. 2020</div>
            Awarded Honor Graduates of UESTC
          </td>
        </tr> -->

<!-- 
        <tr>
          <td width="20%" class="education">
            <img src='images/uestc_icon.jpg' width="100">
          </td>
    
        <td width="75%" valign="middle">
          <p>
          <strong>University of Electronic Science and Technology of China (UESTC), China</strong><br />
          Ph.D in Computer Science and Technology &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2020 - Present <br />
          In Successive Postgraduate and Doctoral Program
          </p>
        </td>
        </tr>
    
    
    
        <tr>
          <td width="20%" class="education">
            <img src='images/uestc_icon.jpg' width="100">
          </td>
    
        <td width="75%" valign="middle">
          <p>
          <strong>University of Electronic Science and Technology of China (UESTC), China</strong><br />
          Bachelor Degree in Software Engineering &nbsp;&nbsp;&nbsp;&nbsp; &bull; Sep. 2016 - Jun. 2020 <br />
          Awarded Honor Graduates of UESTC.
          </p>
        </td>
        </tr>
     -->
      </table>
  </div> 




<!-- The News Section -->
  <div class="w3-container w3-light-grey w3-padding-32" id="news">
   <h2>News</h2>
      <p><li>[2025/04] 2 papers about multimodal learning and video event retrieval accepted by ICMR 2025! 🎉🎉 Congratulations! </p> 
      <p><li>[2025/03] 4 papers about egocentric video analysis and multimodal learning accepted by ICME 2025! 🎉🎉 Congratulations! </p> 
      <p><li>[2025/02] 1 paper about egocentric procedural video verification accepted by CVPR 2025! </p>
      <p><li>[2025/01] Awarded the CAST inaugural Doctoral Student Special Plan of the Young Elite Scientists Sponsorship Program! 🥳</p>
      <p><li>[2024/12] 1 paper about video moment and highlight retrieval accepted by IEEE TCSVT! </p>
      <p><li>[2024/12] Starting my journey in NTU@MReaL Lab! 🙏</p>
      <p><li>[2024/12] 1 paper about time series data augmentation accepted by ICASSP 2025! 🎉🎉Congratulations to Haoran! </p>
      <p><li>[2024/11] 1 paper about audio-visual video parsing accepted by IEEE TNNLS!</p> 
      <!-- <p><li>[2024/07] 2 papers about de-biased video grounding and multimodal fusion accepted by ACM MM 2024! </p> -->
      <!-- <p><li>[2024/05] 1 paper about zero-shot video moment retrieval is accepted by IEEE TMM!</li></p> -->
      <!-- <p><li>[2024/06] 1 paper about multimodal sentiment analysis accepted by IEEE TFS 2024!</li></p> -->
      <!-- <p><li>[2024/04] 1 paper about compositional temporal grounding is accepted by ICMR 2024! Congratulations to Zhuoyuan!</li></p> -->
      <!-- <p><li>[2024/03] 2 papers about multimodal fusion and video content retrieval are accepted by ICME 2024! Congratulations to my co-authors Zixian and Liqing!</li></p> -->
      <!-- <p><li>[2024/03] 1 paper about uncertainty learning in multimodal fusion are accepted by CVPR 2024!</li></p> -->
      <!-- <p><li>[2024/01] 1 paper about composed query based image retrieval is accepted by ACM TOMM! Congratulations to my co-author Shenshen!</li></p> -->
      <!-- <p><li>[2023/08] 1 paper co-author with Shenshen Li about composed query based image retrieval is accepted by IEEE TCSVT! Congratulations to Shenshen!</li></p> -->
      <!-- <p><li>[2023/07] 2 paper about video content retrieval are accepted by ACM MM 2023! </li></p> -->
  </div>


 <!-- The Publications Section -->
  <div class="w3-container w3-padding-32" id="publications">
    <h2>Publications</h2>
      <p class="w3-left-align" style="line-height:200%">
        <!-- Currently, I'm working on multimodal learning in video content understanding, particularly the problems of weakly-supervised learning, robustness, and bias-resisting within it.  -->
        Currently, I'm working on multimodal learning, egocentric video understanding, and applications of vision-language models. 
      </p>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <!-- <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" > -->
          <tr>
          <td width="15%">
            <img src='images/phgc.png'  width="160" height="140", onmouseover="mouseOverPic('images/phgc.png', 'phgc'); phgc.style.visibility='visible'" onmouseout="mouseOutPic('phgc')" class="thumb">
            <div id="phgc" style="visibility:hidden;" class="div1"></div>
          </td>
          <td valign="top" width="75%">
            <strong>Procedural Heterogeneous Graph Completion for Natural Language Task Verification in Egocentric Videos </strong><br>
            <strong>Xun Jiang</strong>, 
            Zhiyi Huang, Xing Xu, Jingkuan Song, Fumin Shen, Heng Tao Shen
            <br>
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2025</strong></em><br>
        
            <a href="https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_PHGC_Procedural_Heterogeneous_Graph_Completion_for_Natural_Language_Task_Verification_CVPR_2025_paper.html">[Paperlink]</a>, <a href="https://github.com/XunCHN/PHGC">[Code]</a><br>
            <!-- <a title="Coming Soon">[Paperlink]</a>, <a title="Coming Soon">[Code]</a><br> -->
            <em>Key Words: Natural Language-based Egocentric Task Verification; Heterogeneous Graph Completion; Multimodal Learning; Procedural Task Understanding</em> <br>
          </td>
        </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <!-- <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" > -->
          <tr>
          <td width="15%">
            <img src='images/caem.jpg'  width="160" height="90", onmouseover="mouseOverPic('images/caem.jpg', 'caem'); caem.style.visibility='visible'" onmouseout="mouseOutPic('caem')" class="thumb">
            <div id="caem" style="visibility:hidden;" class="div1"></div>
          </td>
          <td valign="top" width="75%">
            <strong>Counterfactually Augmented Event Matching for De-biased Temporal Sentence Grounding</strong><br>
            <strong>Xun Jiang</strong>, 
            Zhuoyuan Wei, Shenshen Li, Xing Xu, Jingkuan Song, Heng Tao Shen
            <br>
            <em>ACM Internation Conference on Multimedia, <strong>ACM MM 2024</strong></em><br>
        
            <a href="https://openreview.net/pdf?id=NVzON5t3iz">[Paperlink]</a>, <a href="https://github.com/CFM-MSG/CAEM_Code">[Code]</a><br>
            <em>Key Words: De-biased Video Grounding; Counterfactual Reasoning; Multimodal Learning</em> <br>
          </td>
        </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <!-- <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" > -->
          <tr>
          <td width="15%">
            <img src='images/art.png'  width="160" height="90", onmouseover="mouseOverPic('images/art.png', 'art'); art.style.visibility='visible'" onmouseout="mouseOutPic('art')" class="thumb">
            <div id="art" style="visibility:hidden;" class="div1"></div>
          </td>
          <td valign="top" width="75%">
            <strong>Zero-Shot Video Moment Retrieval with Angular Reconstructive Text Embeddings</strong><br>
            <strong>Xun Jiang</strong>, 
            Xing Xu, Zailei Zhou, Yang Yang, Fumin Shen, Heng Tao Shen  
            <br>
            <em>IEEE Transactions on Multimedia, <strong>TMM 2024</strong></em><br>
        
            <a href="https://ieeexplore.ieee.org/abstract/document/10605104">[Paperlink]</a>, <a href="https://github.com/CFM-MSG/Code_ART">[Code]</a><br>
            <em>Key Words: Video Content Understanding; Weakly-Supervised Learning; CLIP</em> <br>
          </td>
        </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <!-- <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" > -->
          <tr>
            <td width="15%">
              <img src='images/eau.png'  width="160" height="90", onmouseover="mouseOverPic('images/eau.png', 'eau'); eau.style.visibility='visible'" onmouseout="mouseOutPic('eau')" class="thumb">
              <div id="eau" style="visibility:hidden;" class="div1"></div>
            </td>
          <td valign="top" width="75%">
       <strong>Embracing Unimodal Aleatoric Uncertainty for Robust Multimodal Fusion</strong><br>
       Zixian Gao*,
       <strong>Xun Jiang*</strong> (* equal contribution),
       Xing Xu, Fumin Shen, Yujie Li, Heng Tao Shen  
       <br>
            <em>IEEE/CVF Computer Vision and Pattern Recognition Conference, <strong>CVPR 2024</strong></em><br>
        
        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Gao_Embracing_Unimodal_Aleatoric_Uncertainty_for_Robust_Multimodal_Fusion_CVPR_2024_paper.pdf">[Paperlink]</a>, <a href="https://github.com/CFM-MSG/Code_URMF">[Code]</a><br>
            <em>Key Words: Multimodal Learning; Model Robustness; Uncertainty in Deep Learning</em> <br>
            <p></p>
          </td>
        </tr>
       </table>
<!-- 
       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="15%">
              <img src='images/cmap.jpg'  width="160" height="90", onmouseover="mouseOverPic('images/cmap.jpg', 'cmap'); cmap.style.visibility='visible'" onmouseout="mouseOutPic('cmap')" class="thumb">
              <div id="cmap" style="visibility:hidden;" class="div1"></div>
            </td>
          <td valign="top" width="75%">
       <strong>Cross-Modal Attention Preservation with Self-Contrastive Learning for Composed Query-Based Image Retrieval</strong><br>
       Shenshen Li, Xing Xu, <strong>Xun Jiang</strong>, Fumin Shen, Zhe Sun, Andrzej Cichocki
       <br>
            <em>ACM Transactions on Multimedia Computing, Communications, and Applications, <strong>ACM TOMM 2024</strong></em><br>
        
        <a href="https://dl.acm.org/doi/full/10.1145/3639469">[Paperlink]</a>, <a href="https://github.com/CFM-MSG/Code_CMAP">[Code]</a><br>
            <em>Key Words: Cross-modal Retrieval; Composed Query-Based Image Retrieval; Multimodal Learning</em> <br>
            <p></p>
          </td>
        </tr>
       </table> -->

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <!-- <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" > -->
          <tr>
          <td width="15%">
            <img src='images/jsg.png'  width="160" height="90", onmouseover="mouseOverPic('images/jsg.png', 'jsg'); jsg.style.visibility='visible'" onmouseout="mouseOutPic('jsg')" class="thumb">
            <div id="jsg" style="visibility:hidden;" class="div1"></div>
          </td>
          <td valign="top" width="75%">
       <strong>Joint Searching and Grounding: Multi-Granularity Video Content Retrieval</strong><br>
       Zhiguo Chen*,
       <strong>Xun Jiang*</strong> (* equal contribution),
       Xing Xu, Zuo Cao, Yijun Mo, Heng Tao Shen 
       <br>
            <em>ACM Internation Conference on Multimedia, <strong>ACM MM 2023</strong></em><br>
        
        <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612349">[Paperlink]</a>, <a href="https://github.com/CFM-MSG/Code_JSG">[Code]</a><br>
            <em>Key Words: Multimedia Retrieval; Video Content Understanding; Multimodal Learning</em> <br>
            <p></p>
          </td>
        </tr>
       </table>
       
       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <!-- <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" > -->
          <tr>
          <td width="15%">
            <img src='images/MANME.png'  width="160" height="90", onmouseover="mouseOverPic('images/MANME.png', 'manne'); manne.style.visibility='visible'" onmouseout="mouseOutPic('manne')" class="thumb">
            <div id="manne" style="visibility:hidden;" class="div1"></div>
          </td>

          <td valign="top" width="75%">
       <strong>Multi-Grained Attention Network with Mutual Exclusion for Composed Query-Based Image Retrieval</strong><br>
       Shenshen Li, Xing Xu, <strong>Xun Jiang</strong>, Fumin Shen, Xin Liu, Heng Tao Shen
       <br>
            <em>IEEE Transactions on Circuits and Systems for Video Technology, <strong>TCSVT 2023</strong></em><br>
        
        <a href="https://ieeexplore.ieee.org/abstract/document/10225420">[Paperlink]</a>, <a href="https://github.com/CFM-MSG/Code_MANME">[Code]</a><br>
            <em>Key Words: Cross-modal Retrieval; Composed Query-Based Image Retrieval; Multimodal Learning</em> <br>
            <p></p>
          </td>
          
        </tr>
       </table>

       <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <!-- <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" > -->
          <tr>
          <td width="15%">
            <img src='images/cfmr.png'  width="160" height="90", onmouseover="mouseOverPic('images/cfmr.png', 'cfmr'); cfmr.style.visibility='visible'" onmouseout="mouseOutPic('cfmr')" class="thumb">
            <div id="cfmr" style="visibility:hidden;" class="div1"></div>
          </td>
          <td valign="top" width="75%">
       <strong> Faster Video Moment Retrieval with Point-Level Supervision</strong><br>
       <strong>Xun Jiang</strong>,
       Zailei Zhou, Xing Xu, Yang Yang, Guoqing Wang, Heng Tao Shen
       <br>
            <em>ACM Internation Conference on Multimedia, <strong>ACM MM 2023</strong></em><br>
        
        <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612394">[Paperlink]</a>, <a href="https://github.com/CFM-MSG/Code_CFMR">[Code]</a><br>
            <em>Key Words: Video Content Retrieval; Point-level Supervision; Retrieval Efficiency</em> <br>
            <p></p>
          </td>
        </tr>
       </table>
       

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <!-- <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" > -->
      <tr>
      <td width="15%">
        <img src='images/sdn.png'  width="160" height="90", onmouseover="mouseOverPic('images/sdn.png', 'sdn'); sdn.style.visibility='visible'" onmouseout="mouseOutPic('sdn')" class="thumb">
        <div id="sdn" style="visibility:hidden;" class="div1"></div>
      </td>
      <td valign="top" width="75%">
	 <strong>SDN: Semantic Decoupling Network for Temporal Language Grounding</strong><br>
	 <strong>Xun Jiang</strong>,
	 Xing Xu, Jingran Zhang, Fumin Shen, Zuo Cao, Heng Tao Shen
   <br>
        <em>IEEE Transactions on Neural Networks and Learning Systems, <strong>TNNLS 2022</strong></em><br>
		
		<a href="https://ieeexplore.ieee.org/abstract/document/9925990">[Paperlink]</a>, <a href="https://github.com/CFM-MSG/Code_SDN">[Code]</a><br>
        <em>Key Words: Video Content Understanding; Vision-Language; Multimodal Learning</em> <br>
        <p></p>
      </td>
    </tr>
   </table>
   
   
   <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <!-- <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" > -->
      <tr>
      <td width="15%">
        <img src='images/dhhn.png'  width="160" height="90", onmouseover="mouseOverPic('images/dhhn.png', 'dhhn'); dhhn.style.visibility='visible'" onmouseout="mouseOutPic('dhhn')" class="thumb">
        <div id="dhhn" style="visibility:hidden;" class="div1"></div>
      </td>
      <td valign="top" width="75%">
	 <strong>DHHN: Dual Hierarchical Hybrid Network for Weakly-Supervised Audio-Visual Video Parsing</strong><br>
	 <strong>Xun Jiang</strong>,
	 Xing Xu,
	 Zhiguo Chen,
 	 Jingran Zhang,    
 	 Jingkuan Song,    
 	 Fumin Shen,    
 	 Huimin Lu,    
 	 Heng Tao Shen,    
   <br>
        <em>ACM Internation Conference on Multimedia, <strong>ACM MM 2022</strong></em><br>
		
		<a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548309">[Paperlink]</a>, <a href="https://github.com/CFM-MSG/Code_DHHN">[Code]</a><br>
        <em>Key Words: Video Content Understanding; Action Localization; Audio-Visual Learning</em> <br>
        <p></p>
      </td>
    </tr>
   </table>

   <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
      <td width="15%">
        <img src='images/gtlr.png'  width="160" height="90", onmouseover="mouseOverPic('images/gtlr.png', 'gtlr'); gtlr.style.visibility='visible'" onmouseout="mouseOutPic('gtlr')" class="thumb">
        <div id="gtlr" style="visibility:hidden;" class="div1"></div>
      </td>
      <td valign="top" width="75%">
	 <strong>GTLR: Graph-Based Transformer with Language Reconstruction for Video Paragraph Grounding</strong><br>
	 <strong>Xun Jiang</strong>,
	 Xing Xu,
	 Zhiguo Chen,
 	 Jingran Zhang,    
 	 Fumin Shen,    
 	 Zuo Cao,    
 	 Xunliang Cai,    
   <br>
        <em>IEEE International Conference on Multimedia and Expo, <strong>ICME 2022</strong></em><br>
		
		<a href="https://ieeexplore.ieee.org/abstract/document/9859847">[Paperlink]</a>, <a href="https://github.com/CFM-MSG/Code_GTLR">[Code]</a><br>
        <em>Key Words: Video Content Understanding; Vision-Language; Multimodal Learning</em> <br>
        <p></p>
      </td>
    </tr>
   </table> -->


   <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <!-- <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" > -->
      <tr>
      <td width="15%">
        <img src='images/svptr.png'  width="160" height="90", onmouseover="mouseOverPic('images/svptr.png', 'svptr'); svptr.style.visibility='visible'" onmouseout="mouseOutPic('svptr')" class="thumb">
        <div id="svptr" style="visibility:hidden;" class="div1"></div>
      </td>
      <td valign="top" width="75%">
	 <strong>Semi-Supervised Video Paragraph Grounding With Contrastive Encoder</strong><br>
	 <strong>Xun Jiang</strong>,
	 Xing Xu,
 	 Jingran Zhang,    
 	 Fumin Shen,    
 	 Zuo Cao,    
 	 Heng Tao Shen
   <br>
        <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2022</strong></em><br>
		
		<a href="https://ieeexplore.ieee.org/document/9879558">[Paperlink]<a><br>
        <em>Key Words: Video Content Understanding, Semi-Supervised Learning; Multimodal Learning</em> <br>
        <p></p>
      </td>
    </tr>
   </table>
   
    
    <!-- <h3><strong>Large Language Models</strong></h3>
    <div style="background-color: #EEF3FE">
    <ol>
      <p>
        <li><strong>PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language Models</strong>
        <br>
        Jinyi Li, Yihuai Lan, <strong>Lei Wang</strong>, Hao Wang
        <br>
        <em>Preprint</em> 2024 | <a style="color: #447ec9" href="https://arxiv.org/abs/2403.17411">paper</a> | &nbsp<iframe src="https://ghbtns.com/github-btn.html?user=3DAgentWorld&repo=Toolkit-for-Prompt-Compression&type=star&count=true" frameborder="0" scrolling="0" width="150" height="20" title="GitHub" align="bottom" style="position:absolute;Left:30;bottom:-10"></iframe> 
      </p>
    </ol> -->


    <!-- <h3><strong>Others</strong> </h3>
    <div style="background-color: #EEF3FE">
    <ol>

      <p>
      <li><strong>Gradient-Aware Logit Adjustment Loss for Long-Tailed Classifier</strong>
      <br>
      Fan Zhang, Wei Qin, Weijieying Ren, <strong>Lei Wang</strong>, Zetong Chen, Richang Hong
      <br>
      <em>ICASSP</em> 2024 | <a style="color: #447ec9" href="">paper</a> 
      </p>

      <p>
      <li><strong>FlaCGEC: A Chinese Grammatical Error Correction Dataset with Fine-grained Linguistic Annotation</strong>
      <br>
      Hanyue Du, Yike Zhao, Qingyuan Tian, Jiani Wang, <strong>Lei Wang</strong>, Yunshi Lan and Xuesong Lu
      <br>
      <em>CIKM Resource</em> 2023 | <a style="color: #447ec9" href="https://demoleiwang.github.io/HomePage/">paper</a> | Code and Dataset: Coming Soon.
      </p>

      <p>
      <li><strong>A Prompt-Based Topic-Modeling Method for Depression Detection on Low-Resource Data</strong>
      <br>
      Yanrong Guo, Jilong Liu, <strong>Lei Wang</strong>, Wei Qin, Shijie Hao, Richang Hong
      <br>
      <em>IEEE Transactions on Computational Social Systems</em> 2023 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10105271/">paper</a>
      </p>

      <p>
      <li><strong>Deepstyle: User style embedding for authorship attribution of short texts</strong>
      <br>
      Zhiqiang Hu, Roy Ka-Wei Lee, <strong>Lei Wang</strong>, Ee-peng Lim, Bo Dai.
      <br>
      <em>APWeb-WAIM</em> 2020 | <a style="color: #447ec9" href="https://arxiv.org/pdf/2103.11798">paper</a>
      </p>

      <p>
      <li><strong>CRAN: a hybrid CNN-RNN attention-based model for text classification</strong>
      <br>
      Long Guo, Dongxiang Zhang, <strong>Lei Wang</strong>, Han Wang, Bin Cui
      <br>
      <em>Conceptual Modeling</em> 2018 | <a style="color: #447ec9" href="https://link.springer.com/chapter/10.1007/978-3-030-00847-5_42">paper</a>
      </p>
      
    </ol>
   </div> -->
    
  </div> 

 <!-- <div class="w3-container w3-light-grey w3-padding-32" id="service">
    <h2>Services</h2>
      <p><li> Area Chair of <a href="https://icml.cc/Conferences/2023">ICML 2021</a>, <a href="https://nips.cc/Conferences/2022/">NeurIPS 2022</a>, <a href="https://icml.cc/Conferences/2021">ICML 2021</a>, <a href="https://nips.cc/Conferences/2021/">NeurIPS 2021</a>.</p>
      <p><li> Action Editor of <a href="https://jmlr.org/tmlr/">TMLR</a> (Transactions on Machine Learning Research).</p>
      <p><li> Senior Program Committee Members of <a href="https://ijcai-21.org/">IJCAI 2021</a>, <a href="https://www.ijcai20.org/">IJCAI 2020</a> and <a href="https://www.ijcai19.org/program-committee.html">IJCAI 2019</a>.</p>
      <p><li> Journal Reviewers of <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">IEEE T-PAMI</a>, <a href="https://www.springer.com/journal/11263">IJCV</a>, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83">IEEE T-IP</a>, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">IEEE T-NNLS</a>, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">IEEE T-MM</a>, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=69">IEEE T-KDE</a>, etc.</p>
      <p><li> Program Committee Members of ICCV 2021, AAAI 2021, ICLR 2021, NeurIPS 2020, ICML 2020, ECCV 2020, CVPR 2020, ICLR 2020, AAAI 2020, ICCV 2019, CVPR 2019, ICLR 2019, AAAI 2019, IJCAI 2018, AAAI 2018, NeurIPS 2018, etc.</p>
  </div> -->

  <!-- The Awards Section -->
  <div class="w3-container w3-padding-32" id="award">
    <h2>Awards</h2>            
    <p><li> CAST inaugural Doctoral Student Special Plan of the Young Elite Scientists Sponsorship Program, 2025.01</p>
    <p><li> Chinese Scholarship Council Scholarship, 2024.07</p>
    <p><li> Doctoral National Scholarship, 2023.10, 2024.10</p>
    <p><li> "Academic Newcomer" Ph.D Student Honor Award of UESTC, 2023.04</p>
    <p><li> ICME Best Student Paper, 2022.07</p>
    <p><li> "Academic Youth" Postgraduate Student Honor Award of UESTC, 2022.04</p>
    <p><li> Honor Graduates of UESTC, 2020.06</p>
  </div>

  <!-- Student Collaboration/Mentoring -->
  <!-- <div class="w3-container w3-padding-32" id="award">
    <h2>Student Collaboration/Mentoring</h2>
  </div>  
   -->

  <div class="w3-light-grey w3-center w3-padding-24", title="Last updated on Apr., 2025">
    This template is borrowed from Lei Wang's website <a href="https://github.com/demoleiwang/HomePage">source code</a>. Many thanks to Lei Wang.

  <!-- Default Statcounter code for Lei Wang's Homepage
  https://demoleiwang.github.io/HomePage/
  No.
  <script type="text/javascript">
  var sc_project=12347113; 
  var sc_invisible=0; 
  var sc_security="21aca5d1"; 
  var sc_https=1; 
  var scJsHost = "https://";
  document.write("<sc"+"ript type='text/javascript' src='" + scJsHost+
  "statcounter.com/counter/counter.js'></"+"script>");
  </script> Visitor Since Feb 2022. Powered by <a href="https://www.w3schools.com/w3css/default.asp" title="W3.CSS" target="_blank" class="w3-hover-opacity">w3.css</a>
  <noscript>
    <div class="statcounter"><a title="Web Analytics Made Easy -
  StatCounter" href="https://statcounter.com/" target="_blank"><img
  class="statcounter" src="https://c.statcounter.com/12347113/0/21aca5d1/0/"
  alt="Web Analytics Made Easy - StatCounter"></a></div>
  </noscript> -->
  <!-- End of Statcounter Code -->

  </div>

  <!-- End page content -->
</div>

<script>
// Accordion 
function myAccFunc() {
  var x = document.getElementById("demoAcc");
  if (x.className.indexOf("w3-show") == -1) {
    x.className += " w3-show";
  } else {
    x.className = x.className.replace(" w3-show", "");
  }
}

// Click on the "Jeans" link on page load to open the accordion for demo purposes
document.getElementById("myBtn").click();


// Open and close sidebar
function w3_open() {
  document.getElementById("mySidebar").style.display = "block";
  document.getElementById("myOverlay").style.display = "block";
}
 
function w3_close() {
  document.getElementById("mySidebar").style.display = "none";
  document.getElementById("myOverlay").style.display = "none";
}
</script>


<script language=JavaScript>
  function mouseOutPic(id_name)   
  {
  if(window.event.toElement.className!="thumb") 
    document.getElementById(id_name).style.visibility = "hidden";
  }
  function mouseOverPic(imgSrc, id_name)    
  {
  var seeBig = document.getElementById(id_name);    
  var newImg = document.createElement("img");    
  newImg = "<img src=\'" + imgSrc + "\'/>";    
  //alert(imgSrc);
  seeBig.innerHTML = newImg;    
  }
</script>

</body>
</html>
